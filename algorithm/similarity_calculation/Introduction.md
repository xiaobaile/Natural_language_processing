# 介绍
- 在自然语言处理中，文本相似度是一种老生常谈而又应用广泛的`基础算法`模块，
可用于地址标准化中计算与标准地址库中最相似的地址，也可用于问答系统中计算与用户输入问题最相近的问题及其答案，
还可用于搜索中计算与输入相近的结果，扩大搜索召回，等等。

## 字符串相似度
- 字符串相似度指的是比较两个文本相同字符个数，从而得出其相似度。 
python为我们提供了一个`difflib`包用于计算两个文本序列的匹配程度，
我们可以将其视为两个文本字符串的相似度，其代码实现很简单，
如果需要得到两个文本之间相同部分或不同部分，可参考**string_similarity.py**.

## simhash相似度
- simhash最早是由google在文章《detecting near-duplicates for web crawling》中提出的一种用于`网页去重`的算法。
simhash是一种局部敏感hash，计算速度快，对海量网页文本可实现快速处理。
- 1.**分词**，把需要判断文本分词形成这个文章的特征单词。最后形成去掉噪音词的单词序列并为每个词加上权重，我们假设权重分为5个级别（1~5）。比如：“ 美国“51区”雇员称内部有9架飞碟，曾看见灰色外星人 ” ==> 分词后为 “ 美国（4） 51区（5） 雇员（3） 称（1） 内部（2） 有（1） 9架（3） 飞碟（5） 曾（1） 看见（3） 灰色（4） 外星人（5）”，括号里是代表单词在整个句子里重要程度，数字越大越重要。
- 2.**hash**，通过hash算法把每个词变成hash值，比如“美国”通过hash算法计算为 100101,“51区”通过hash算法计算为 101011。这样我们的字符串就变成了一串串数字，还记得文章开头说过的吗，要把文章变为数字计算才能提高相似度计算性能，现在是降维过程进行时。
- 3.**加权**，通过 2步骤的hash生成结果，需要按照单词的权重形成加权数字串，比如“美国”的hash值为“100101”，通过加权计算为“4 -4 -4 4 -4 4”；“51区”的hash值为“101011”，通过加权计算为 “ 5 -5 5 -5 5 5”。
- 4.**合并**，把上面各个单词算出来的序列值累加，变成只有一个序列串。比如 “美国”的 “4 -4 -4 4 -4 4”，“51区”的 “ 5 -5 5 -5 5 5”， 把每一位进行累加， “4+5 -4+-5 -4+5 4+-5 -4+5 4+5” ==》 “9 -9 1 -1 1 9”。这里作为示例只算了两个单词的，真实计算需要把所有单词的序列串累加。
- 5.**降维**，把4步算出来的 “9 -9 1 -1 1 9” 变成 0 1 串，形成我们最终的simhash签名。 如果每一位大于0 记为 1，小于0 记为 0。最后算出结果为：“1 0 1 0 1 1”。
- simhash的主要思想是将高维的特征向量（文本可转换为高维向量表示）映射成低维的特征向量，通过计算两个向量的汉明距离(Hamming Distance)来确定文本的相似度。 其中，汉明距离，表示在两个等长字符串中对应位置不同字符的个数。如，1011100 与 1001000 之间的汉明距离是 2。而字符串的编辑距离则是汉明距离的扩展

## word2vec相似度
- word2vec是对词语进行向量化的一种`无监督算法`.word2vec相似度是指利用word2vec算法将文本向量化，进而利用`余弦距离`计算两个向量的余弦相似度作为两字符串的相似度。
- word2vec对文本的向量化是将文本分词后，得到各词语的向量化表示，然后对向量的每个维度进行加权相加，形成文本向量，进而可利用余弦距离计算文本的相似度。

## 神经网络相似度
- 利用神经网络进行相似度计算的一种思路是将输入X编码为中间向量V，然后对中间结果进行解码得到输出Y，其中损失函数的计算方式就是尽可能减少X与Y之间的偏差，理想情况就是中间向量V能完全解码还原为原始输入X。网络训练完成后，我们也就得到了输入句子所表示的语义特征向量。

根据上述思路，我们自然可以联想到最基础的自编码器(AutoEncoder, AE)，当然还有其他更复杂的自编码器（ 如：栈式自编码器(Stacked Autoencoder, SAE)、变分自编码器(Variational auto-encoder, VAE)等）。

AutoEncoder能很好的编码句子中所包含的语义信息，可以在一定程度上解决字符串相似度计算中所缺乏的语义理解问题和word2vec相似度计算中所缺乏的词序问题。本文基于tensorflow实现了基础版本的自编码器，模型代码如下，完整的项目代码可参考
- 